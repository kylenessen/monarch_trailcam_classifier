---
title: "Sampling Interval Analysis"
author: "Kyle Nessen"
date: today
format: 
    html:
        embed-resources: true
execute:
    echo: false
    warning: false
    message: false
---

## Background 

This report was generated at the request of Francis to answer the question of what is an appropriate sampling interval for the Vandenberg space-for-space camera study. The goal here is to empirically interrogate if subsampling the photo data is functionally equivalent to the full resolution of images. My motivation for sub-sampling is that labeling these photos is labor-intensive, and I suspect that such a high frequency of 10-minute intervals is introducing unneeded noise. And so if we can reduce the number of images that we label, we can process these images much sooner. Even switching from 30 minute intervals from 10 minute intervals will reduce the total number of images we have to process by three times.However, we should be careful that we're not throwing away necessary information to answer the question at hand, which this report is intended to shed some light on.

## Data Preperation

I won't go into the full detail of how I prepared and cleaned the data, but instead I'll just give a quick summary to provide additional context for the results that will follow. With that said, I have all the code and the full analysis script that I'm happy to share if someone reading this would like to see the specifics.

### Monarch Count Data Preparation
- Loaded SC1 and SC2 datasets with 5-minute intervals; reduced to 10-minute intervals by keeping every other row.
  - These two deployments are the only in the full dataset with this interval, which is why I chose to reduce to 10 minutes.
- Parsed timestamps and sorted data by deployment and time.
- Appended SC4 data and re-sorted the full 10 min dataset.

### Nighttime Data Removal
- Defined specific night periods for each deployment where labelers copy and pasted last valid count value
- Removed observations during these periods to avoid duplicated or unreliable counts.

### Subsampling
- Created datasets at 10, 20, 30, 60, 90, and 120-minute intervals by retaining every nth row within each deployment group.

### Monarch Change Calculation
- Calculated changes in butterfly counts between time points.
- Replaced zero changes with 0.1 and applied a signed log transformation to prepare for modeling.
  - This was done based on initial modeling and poor performance. Log transforming helped.

### Wind Data Integration
- Loaded wind data and aligned it with butterfly observation intervals based on deployment and timestamps.
- Calculated average wind speed, average and max gusts, and number of wind observations per interval.

## Modeling

based on my conversation with Francis we decided to try a information theoritic approach. essentially build the sub-sampled datasets, run the same model on each, and compare AIC scores. Below is some information on how I did that.

#### Model Preparation
- Defined `monarch_change` as the response variable (log transformed).
- Selected wind metrics (`avg_wind_speed_mph`, `avg_wind_gust_mph`, `max_wind_gust_mph`) as fixed effects.
- Included `deployment_id` as a random effect to account for variation between deployments.
- Scaled all wind predictors for comparability.
- Converted timestamps to numeric values to model temporal autocorrelation.

#### Model Fitting
- Used `lme()` from the `nlme` package to fit a linear mixed-effects model.
- Included a first-order autoregressive structure (`corAR1`) to account for temporal correlation within deployments.
- Applied the model to each subsampled dataset (10 to 120 minutes).
- Captured model convergence status, AIC, fixed and random effects, and autocorrelation parameter.

## AIC Comparison
Below is the AIC scores for each sub-sample dataset along with a chart. It's clear from these results that the longest interval performs the best model. But I'm cautious to take these interpretations because I think we are misusing AIC as a tool of comparison because we are effectively changing the data set for each model. I believe the AIC score is very sensitive to the number of data points, which may explain why we see such a predictable decrease as we increase the interval.

```{r}
# Read and display the model comparison results
model_comparison <- read.csv("figures/model_comparison.csv")

# Create a nicely formatted table using knitr::kable
knitr::kable(model_comparison,
    col.names = c("Interval (minutes)", "AIC Score", "Number of Observations", "Model Converged"),
    digits = 2,
    caption = "Model comparison across different sampling intervals"
)
```

![AIC scores for all sub-sampled datasets.](figures/aic_scores.png)

## 30 Min Model Results

