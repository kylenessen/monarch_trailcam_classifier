## Prep monarch count data

Here I am loading in the available count data so far. SC1 and SC2 are the only deployments with a 5 minute interval, so I am removing every other row initially to get it to 10 mins. Then I append SC4, which is more typical.

```{r}
library(tidyverse)

data_directory <- "data/"
csv_files <- c("SC1.csv", "SC2.csv") # Load the 5 min interval datasets

df <- csv_files %>%
    map_df(~ read_csv(file.path(data_directory, .))) %>%
    mutate(time = ymd_hms(timestamp, tz = "America/Los_Angeles")) %>%
    arrange(deployment_id, time) %>%
    group_by(deployment_id) %>%
    filter(row_number() %% 2 == 1) %>% # Keep every other row (odd numbered rows)
    ungroup() %>%
    as_tibble()

# Read SC4 data and process it similarly to the other files
SC4 <- read.csv("data/SC4.csv") %>%
    mutate(time = ymd_hms(timestamp, tz = "America/Los_Angeles")) %>%
    as_tibble()

# Combine all data
df <- bind_rows(df, SC4) %>%
    arrange(deployment_id, time) %>%
    mutate(
        day = as.Date(time, tz = "America/Los_Angeles"),
        observation_period = paste(deployment_id, day)
    )
```

## Remove night photos

Night photos are difficult to see the butterflies, so we simply copy and pasted the last valid count until we can see them again. This will affect our stats, of course, so I am going to remove them here.

```{r}
# Define night periods for each deployment
# Format: "YYYY-MM-DD HH:MM:SS"
night_periods <- list(
    SC1 = list(
        # Night 1
        list(
            start = "20231117174001",
            end = "20231118062001"
        ),
        # Night 2
        list(
            start = "20231118172501",
            end = "20231119061501"
        ),
        # Night 3
        list(
            start = "20231119171001",
            end = "20231120062001"
        )
    ),
    SC2 = list(
        # Night 1
        list(
            start = "20231117172501",
            end = "20231118062001"
        ),
        # Night 2
        list(
            start = "20231118171501",
            end = "20231119061501"
        )
    ),
    SC4 = list(
        # Night 1
        list(
            start = "20231203171001",
            end = "20231204063001"
        )
    )
)

# Function to check if a timestamp falls within any night period
is_night_time <- function(timestamp, deployment_periods) {
    any(sapply(deployment_periods, function(period) {
        timestamp >= ymd_hms(period$start, tz = "America/Los_Angeles") &
            timestamp <= ymd_hms(period$end, tz = "America/Los_Angeles")
    }))
}

# Filter out night periods from the dataframe
df <- df %>%
    group_by(deployment_id) %>%
    filter(!mapply(
        function(t, d) is_night_time(t, night_periods[[d]]),
        time,
        deployment_id
    )) %>%
    ungroup()

```

## Time series for SC1 and SC2

```{r}
sc1 <- df %>% filter(deployment_id == "SC1")
ggplot(df, aes(x = time, y = count)) +
    geom_point(alpha = 0.5) +
    geom_smooth() +
    labs(
        title = "Monarch Counts Over Time - SC1",
        x = "Time",
        y = "Count",
        caption = "Source: Trailcam Images"
    ) +
    theme_minimal()
```


## Generate the subsampled datasets

Here I split out the original df into my test dataset. I'm going to go with 10, 20, 30, 60, 90, and 120 minutes. They will be named accordingly.
For each interval, I'll generate all possible permutations to get a more robust analysis.

```{r}
# Function to generate all possible permutations for a given interval
generate_permutations <- function(df, interval) {
    permutations <- list()
    for (offset in 1:interval) {
        permutations[[offset]] <- df %>%
            group_by(deployment_id) %>%
            filter((row_number() - offset) %% interval == 0) %>%
            ungroup()
    }
    names(permutations) <- paste0("perm_", 1:interval)
    return(permutations)
}

# 10 minutes (keep all rows)
df10_list <- list(df)
names(df10_list) <- "perm_1"

# 20 minutes (keep every other row)
df20_list <- generate_permutations(df, 2)

# 30 minutes (keep every third row)
df30_list <- generate_permutations(df, 3)

# 60 minutes (keep every sixth row)
df60_list <- generate_permutations(df, 6)

# 90 minutes (keep every ninth row)
df90_list <- generate_permutations(df, 9)

# 120 minutes (keep every twelfth row)
df120_list <- generate_permutations(df, 12)

# Store all permutation lists in a named list for easier processing
all_permutations <- list("10" = df10_list, "20" = df20_list, "30" = df30_list, "60" = df60_list, "90" = df90_list, "120" = df120_list)
```

## Calculate change in monarch counts for each dataset

Here I define a function to calculate the change in monarch butterfly abundance and then apply it to all of my datasets. Changes of zero are replaced with 0.1 before log transformation.

```{r}
# Function to calculate monarch count changes
calculate_monarch_changes <- function(df) {
    df %>%
        arrange(deployment_id, time) %>%
        group_by(deployment_id) %>%
        mutate(
            prev_count = lag(count),
            raw_change = count - prev_count,
            # Replace zero changes with 0.1 before log transform
            change_for_log = ifelse(raw_change == 0, 0.1, raw_change),
            monarch_change = log(abs(change_for_log)) * sign(change_for_log),
            time_diff_minutes = as.numeric(difftime(time, lag(time), units = "mins"))
        ) %>%
        ungroup()
}

# Apply the function to each permutation in each interval
all_permutations <- map(all_permutations, function(interval_list) {
    map(interval_list, calculate_monarch_changes)
})

# For backward compatibility, create single dataframes for each interval using first permutation
df10 <- all_permutations[["10"]][[1]]
df20 <- all_permutations[["20"]][[1]]
df30 <- all_permutations[["30"]][[1]]
df60 <- all_permutations[["60"]][[1]]
df90 <- all_permutations[["90"]][[1]]
df120 <- all_permutations[["120"]][[1]]
```

## Calculate wind parameters between intervals

Here I calculate wind metric summaries between butterfly observations

```{r}
# Load wind data
wind <- read_csv("https://raw.githubusercontent.com/kylenessen/masters-wind-data/main/vsfb/vsfb_wind_data.csv")

# Helper function to create empty wind summary
create_empty_wind_summary <- function() {
    tibble(
        avg_wind_speed_mph = NA_real_,
        avg_wind_gust_mph = NA_real_,
        max_wind_gust_mph = NA_real_,
        n_wind_obs = 0L
    )
}

# Function to add interval timestamps to a dataframe
add_intervals <- function(df) {
    df %>%
        arrange(deployment_id, time) %>%
        group_by(deployment_id) %>%
        mutate(
            interval_end = time,
            interval_start = lag(time)
        ) %>%
        ungroup()
}

# Function to calculate wind metrics between intervals
calculate_wind_metrics <- function(deploy_id, start_time, end_time) {
    # Return NA values if start time is missing
    if (is.na(start_time)) {
        return(create_empty_wind_summary())
    }

    # Filter wind data for the interval
    wind_interval <- wind %>%
        filter(
            deployment_id == deploy_id,
            time >= start_time,
            time <= end_time
        )

    # Return NA values if no wind observations found
    if (nrow(wind_interval) == 0) {
        return(create_empty_wind_summary())
    }

    # Calculate wind metrics
    wind_interval %>%
        summarize(
            avg_wind_speed_mph = mean(speed_mph, na.rm = TRUE),
            avg_wind_gust_mph = mean(gust_mph, na.rm = TRUE),
            max_wind_gust_mph = max(gust_mph, na.rm = TRUE),
            n_wind_obs = n()
        )
}

# Main function to add wind metrics to a dataframe
add_wind_metrics <- function(df) {
    # Add interval timestamps
    df_with_intervals <- add_intervals(df)

    # Calculate wind summaries
    wind_summaries <- pmap_dfr(
        list(
            deploy_id = df_with_intervals$deployment_id,
            start_time = df_with_intervals$interval_start,
            end_time = df_with_intervals$interval_end
        ),
        calculate_wind_metrics
    )

    # Combine original data with wind summaries
    bind_cols(df_with_intervals, wind_summaries)
}

# Apply wind metrics to each permutation in each interval
all_permutations <- map(all_permutations, function(interval_list) {
    map(interval_list, add_wind_metrics)
})

# Update single dataframes for backward compatibility
df10 <- all_permutations[["10"]][[1]]
df20 <- all_permutations[["20"]][[1]]
df30 <- all_permutations[["30"]][[1]]
df60 <- all_permutations[["60"]][[1]]
df90 <- all_permutations[["90"]][[1]]
df120 <- all_permutations[["120"]][[1]]
```

## Modeling

The fun part! Let's build a model. `monarch_change` will be the response variable. Our fixed effects are the wind variables: `avg_wind_speed_mph`, `avg_wind_gust_mph`, `max_wind_gust_mph`. Our only random effect is `deployment_id`. I want to also correct for temporal autocorrelation using either `time` (if datetime objects are useful) or `timestamp` if some numerical value must be used.

Let's build this model for all of our subsampled datasets and compare AIC scores

```{r}
# Load required packages for mixed effects models and temporal autocorrelation
library(lme4)
library(nlme)
library(lubridate)

# Function to prepare data for modeling by removing NA values and scaling predictors
prepare_data <- function(df) {
    df %>%
        filter(
            !is.na(monarch_change), !is.na(avg_wind_speed_mph),
            !is.na(avg_wind_gust_mph), !is.na(max_wind_gust_mph)
        ) %>%
        mutate(
            # Scale wind variables
            avg_wind_speed_scaled = scale(avg_wind_speed_mph),
            avg_wind_gust_scaled = scale(avg_wind_gust_mph),
            max_wind_gust_scaled = scale(max_wind_gust_mph),
            # Create numeric time variable for autocorrelation
            time_numeric = as.numeric(time)
        )
}

# Function to fit model and extract AIC
fit_model <- function(df, interval) {
    # Prepare data
    model_data <- prepare_data(df)

    # Fit model with temporal correlation structure
    model <- try(
        {
            lme(monarch_change ~ avg_wind_speed_scaled + avg_wind_gust_scaled + max_wind_gust_scaled,
                random = ~ 1 | deployment_id,
                correlation = corAR1(form = ~ time_numeric | deployment_id),
                data = model_data,
                method = "REML"
            )
        },
        silent = TRUE
    )

    # Extract model results
    if (!inherits(model, "try-error")) {
        # Extract fixed effects
        fixed_effects <- fixef(model)
        # Extract random effects variance
        random_effects_var <- as.numeric(VarCorr(model)[1, 1])
        # Get correlation parameter
        corr_param <- coef(model$modelStruct$corStruct, unconstrained = FALSE)
        # Calculate AIC
        aic <- AIC(model)

        # Return results
        list(
            interval = interval,
            converged = TRUE,
            AIC = aic,
            fixed_effects = fixed_effects,
            random_effects_var = random_effects_var,
            correlation = corr_param,
            n_obs = nrow(model_data),
            model = model
        )
    } else {
        # Return NA values if model fails
        list(
            interval = interval,
            converged = FALSE,
            AIC = NA,
            fixed_effects = NA,
            random_effects_var = NA,
            correlation = NA,
            n_obs = nrow(model_data),
            model = NA
        )
    }
}

# Fit models for each dataset
results_10 <- fit_model(df10, 10)
results_20 <- fit_model(df20, 20)
results_30 <- fit_model(df30, 30)
results_60 <- fit_model(df60, 60)
results_90 <- fit_model(df90, 90)
results_120 <- fit_model(df120, 120)

# Combine results into a data frame
model_comparison <- tibble(
    Interval = c(10, 20, 30, 60, 90, 120),
    AIC = c(
        results_10$AIC, results_20$AIC, results_30$AIC,
        results_60$AIC, results_90$AIC, results_120$AIC
    ),
    N = c(
        results_10$n_obs, results_20$n_obs, results_30$n_obs,
        results_60$n_obs, results_90$n_obs, results_120$n_obs
    ),
    Converged = c(
        results_10$converged, results_20$converged, results_30$converged,
        results_60$converged, results_90$converged, results_120$converged
    )
)

# Display model comparison results
print("Model Comparison Results:")
print(model_comparison)

# Save model comparison results to CSV
write.csv(model_comparison, "figures/model_comparison.csv", row.names = FALSE)

# Plot AIC scores
aic_plot <- ggplot(model_comparison, aes(x = Interval, y = AIC)) +
    geom_line() +
    geom_point() +
    labs(
        title = "AIC Scores by Sampling Interval",
        x = "Sampling Interval (minutes)",
        y = "AIC Score"
    ) +
    theme_minimal()

# Save the plot
ggsave("figures/aic_scores.png", aic_plot, width = 8, height = 6, dpi = 300)

# Display detailed results for the best model (lowest AIC)
best_model_idx <- which.min(model_comparison$AIC)
best_interval <- model_comparison$Interval[best_model_idx]
best_model <- get(paste0("results_", best_interval))

print(paste("\nBest Model (", best_interval, "minute interval):"))
print(summary(best_model$model))

# Plot residuals vs fitted values for best model
plot(best_model$model, which = 1)
```

## 


```{r}
# Calculate RMSE between full dataset and subsampled datasets
calculate_rmse <- function(full_data, subsampled_data) {
    # Join datasets by timestamp
    comparison <- full_data %>%
        select(deployment_id, time, observation_period, count) %>%
        rename(full_count = count) %>%
        inner_join(
            subsampled_data %>%
                select(deployment_id, time, observation_period, count) %>%
                rename(subsampled_count = count),
            by = c("deployment_id", "time", "observation_period")
        )

    # Calculate RMSE
    sqrt(mean((comparison$full_count - comparison$subsampled_count)^2, na.rm = TRUE))
}

# For observation period averages RMSE
calculate_period_rmse <- function(full_data, subsampled_data) {
    # Calculate observation period averages
    full_period <- full_data %>%
        group_by(observation_period) %>%
        summarize(avg_count = mean(count, na.rm = TRUE), .groups = "drop")

    sub_period <- subsampled_data %>%
        group_by(observation_period) %>%
        summarize(avg_count = mean(count, na.rm = TRUE), .groups = "drop")

    # Join and calculate RMSE
    comparison <- full_period %>%
        inner_join(sub_period,
            by = "observation_period",
            suffix = c("_full", "_subsampled")
        )

    sqrt(mean((comparison$avg_count_full - comparison$avg_count_subsampled)^2, na.rm = TRUE))
}

# Function to calculate RMSE with single outlier removal
calculate_rmse_with_outlier_removal <- function(full_data, subsampled_data) {
    # Join datasets by timestamp
    comparison <- full_data %>%
        select(deployment_id, time, observation_period, count) %>%
        rename(full_count = count) %>%
        inner_join(
            subsampled_data %>%
                select(deployment_id, time, observation_period, count) %>%
                rename(subsampled_count = count),
            by = c("deployment_id", "time", "observation_period")
        )

    # Calculate differences and z-scores
    differences <- comparison$full_count - comparison$subsampled_count
    z_scores <- scale(differences)

    # Find points > 2 standard deviations
    outliers <- which(abs(z_scores) > 2)

    if (length(outliers) > 0) {
        # Find the most extreme outlier
        most_extreme <- outliers[which.max(abs(z_scores[outliers]))]
        # Remove the most extreme outlier
        differences <- differences[-most_extreme]
    }

    # Calculate RMSE without the outlier
    sqrt(mean(differences^2, na.rm = TRUE))
}

# For observation period averages RMSE with outlier removal
calculate_period_rmse_with_outlier_removal <- function(full_data, subsampled_data) {
    # Calculate observation period averages
    full_period <- full_data %>%
        group_by(observation_period) %>%
        summarize(avg_count = mean(count, na.rm = TRUE), .groups = "drop")

    sub_period <- subsampled_data %>%
        group_by(observation_period) %>%
        summarize(avg_count = mean(count, na.rm = TRUE), .groups = "drop")

    # Calculate differences and remove single most extreme outlier
    differences <- full_period$avg_count - sub_period$avg_count
    z_scores <- scale(differences)

    # Find points > 2 standard deviations
    outliers <- which(abs(z_scores) > 2)

    if (length(outliers) > 0) {
        # Find the most extreme outlier
        most_extreme <- outliers[which.max(abs(z_scores[outliers]))]
        # Remove the most extreme outlier
        differences <- differences[-most_extreme]
    }

    # Calculate RMSE without the outlier
    sqrt(mean(differences^2, na.rm = TRUE))
}

# Calculate RMSE for all permutations
calculate_all_permutation_rmse <- function(full_data, interval_list, interval) {
    map_dfr(names(interval_list), function(perm_name) {
        tibble(
            Interval = interval,
            Permutation = as.numeric(gsub("perm_", "", perm_name)),
            Raw_RMSE = calculate_rmse(full_data, interval_list[[perm_name]]),
            Raw_RMSE_No_Outlier = calculate_rmse_with_outlier_removal(full_data, interval_list[[perm_name]]),
            Period_RMSE = calculate_period_rmse(full_data, interval_list[[perm_name]]),
            Period_RMSE_No_Outlier = calculate_period_rmse_with_outlier_removal(full_data, interval_list[[perm_name]])
        )
    })
}

# Calculate RMSE for all intervals except 10 (which is our reference)
rmse_results <- map_dfr(names(all_permutations)[2:6], function(interval) {
    calculate_all_permutation_rmse(
        all_permutations[["10"]][[1]],
        all_permutations[[interval]],
        as.numeric(interval)
    )
})

# Calculate summary statistics for each interval
rmse_summary <- rmse_results %>%
    group_by(Interval) %>%
    summarize(
        Mean_Raw_RMSE = mean(Raw_RMSE),
        SD_Raw_RMSE = sd(Raw_RMSE),
        Min_Raw_RMSE = min(Raw_RMSE),
        Max_Raw_RMSE = max(Raw_RMSE),
        Mean_Raw_RMSE_No_Outlier = mean(Raw_RMSE_No_Outlier),
        SD_Raw_RMSE_No_Outlier = sd(Raw_RMSE_No_Outlier),
        Min_Raw_RMSE_No_Outlier = min(Raw_RMSE_No_Outlier),
        Max_Raw_RMSE_No_Outlier = max(Raw_RMSE_No_Outlier),
        Mean_Period_RMSE = mean(Period_RMSE),
        SD_Period_RMSE = sd(Period_RMSE),
        Min_Period_RMSE = min(Period_RMSE),
        Max_Period_RMSE = max(Period_RMSE),
        Mean_Period_RMSE_No_Outlier = mean(Period_RMSE_No_Outlier),
        SD_Period_RMSE_No_Outlier = sd(Period_RMSE_No_Outlier),
        Min_Period_RMSE_No_Outlier = min(Period_RMSE_No_Outlier),
        Max_Period_RMSE_No_Outlier = max(Period_RMSE_No_Outlier),
        N_Permutations = n()
    )
```


```{r}
# Create visualization of RMSE with only Period Averages
rmse_plot_original <- ggplot(rmse_results, aes(x = Interval, y = Period_RMSE)) +
    # Add individual points for each permutation
    geom_point(alpha = 0.5, color = "#E94057") +
    # Add mean line
    stat_summary(fun = mean, geom = "line", color = "#E94057", size = 1) +
    # Add error bars showing standard deviation
    stat_summary(
        fun.data = mean_sdl,
        geom = "errorbar",
        color = "#E94057",
        width = 5
    ) +
    labs(
        title = "Relative RMSE by Sampling Interval",
        subtitle = "Points show individual permutations, line shows mean with error bars",
        x = "Sampling Interval (minutes)",
        y = "Period RMSE"
    ) +
    theme_minimal() +
    theme(
        panel.grid.minor = element_blank(),
        text = element_text(size = 12),
        plot.subtitle = element_text(size = 10, color = "gray50")
    )

# Create visualization comparing original and outlier-removed RMSE
rmse_plot_comparison <- rmse_results %>%
    # Reshape data for comparison
    select(Interval, Period_RMSE, Period_RMSE_No_Outlier) %>%
    pivot_longer(
        cols = c(Period_RMSE, Period_RMSE_No_Outlier),
        names_to = "metric",
        values_to = "rmse"
    ) %>%
    mutate(
        metric = factor(
            metric,
            levels = c("Period_RMSE", "Period_RMSE_No_Outlier"),
            labels = c("Original", "Single Outlier Removed")
        )
    ) %>%
    ggplot(aes(x = Interval, y = rmse, color = metric)) +
    # Add individual points for each permutation
    geom_point(alpha = 0.5, position = position_dodge(width = 3)) +
    # Add mean line
    stat_summary(
        fun = mean,
        geom = "line",
        size = 1,
        aes(group = metric)
    ) +
    # Add error bars showing standard deviation
    stat_summary(
        fun.data = mean_sdl,
        geom = "errorbar",
        width = 5,
        position = position_dodge(width = 3)
    ) +
    scale_color_manual(values = c("#E94057", "#4A90E2")) +
    labs(
        title = "Relative RMSE by Sampling Interval: Original vs Outlier Removed",
        subtitle = "Single most extreme outlier (>2 SD) removed per permutation if present",
        x = "Sampling Interval (minutes)",
        y = "Period RMSE",
        color = "Method"
    ) +
    theme_minimal() +
    theme(
        panel.grid.minor = element_blank(),
        text = element_text(size = 12),
        plot.subtitle = element_text(size = 10, color = "gray50"),
        legend.position = "bottom"
    )

# Save summary statistics
write.csv(rmse_summary, "figures/rmse_summary.csv", row.names = FALSE)
print(rmse_summary)

# Save both plots
ggsave("figures/rmse_by_interval.png", rmse_plot_original, width = 8, height = 6, dpi = 300)
ggsave("figures/rmse_by_interval_comparison.png", rmse_plot_comparison, width = 10, height = 7, dpi = 300)

# Print the number of permutations for each interval
cat("\nNumber of permutations per interval:\n")
print(table(rmse_results$Interval))
```

```{r}
# Create a list of dataframes with different intervals and add an interval label
df_list <- list(
    df10 %>% mutate(interval = "10 min"),
    df20 %>% mutate(interval = "20 min"),
    df30 %>% mutate(interval = "30 min"),
    df60 %>% mutate(interval = "60 min"),
    df90 %>% mutate(interval = "90 min"),
    df120 %>% mutate(interval = "120 min")
)

# Combine all dataframes
all_df <- bind_rows(df_list)

# Get unique observation periods
observation_periods <- unique(df10$observation_period)

# Create a list to store plots
plot_list <- list()

# Create a faceted plot for each observation period
for (period in observation_periods) {
    # Filter data for this observation period
    period_data <- all_df %>%
        filter(observation_period == period)

    # Create the plot with facets by interval
    p <- ggplot(period_data, aes(x = time, y = count)) +
        # Add points and lines without color (since we're faceting)
        geom_line(size = 0.8) +
        geom_point(size = 2.5) +
        # Facet by sampling interval
        facet_wrap(~interval, ncol = 2) +
        # Improve labels
        labs(
            title = paste("Monarch Counts for", period),
            subtitle = "Each panel shows a different sampling interval",
            x = "Time",
            y = "Count"
        ) +
        # Better theme
        theme_minimal() +
        theme(
            strip.background = element_rect(fill = "lightgray", color = NA),
            strip.text = element_text(face = "bold", size = 12),
            axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
            panel.spacing = unit(1, "lines"),
            panel.border = element_rect(color = "gray", fill = NA, size = 0.5)
        )

    # Store the plot
    plot_list[[period]] <- p
}

# Print all plots
for (period in observation_periods) {
    print(plot_list[[period]])
}

# Save each plot
for (period in observation_periods) {
    ggsave(
        filename = paste0("figures/faceted_", gsub(" ", "_", period), ".png"),
        plot = plot_list[[period]],
        width = 10,
        height = 8,
        dpi = 300
    )
}
```



